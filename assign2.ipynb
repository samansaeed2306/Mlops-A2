{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_data(url):\n",
    "    \"\"\"\n",
    "    Scrapes links, titles, and descriptions from a webpage.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing lists of links, titles, and descriptions.\n",
    "    \"\"\"\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract links, titles, and descriptions\n",
    "    scraped_links = []\n",
    "    scraped_titles = []\n",
    "    scraped_descriptions = []\n",
    "\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href and href.startswith('http'):\n",
    "            scraped_links.append(href)\n",
    "            title = link.get_text(strip=True)\n",
    "            scraped_titles.append(title)\n",
    "            # Extract description if available\n",
    "            description = link.find_next_sibling(string=True)\n",
    "            if description and len(description.strip()) > 0:\n",
    "                scraped_descriptions.append(description.strip())\n",
    "            else:\n",
    "                scraped_descriptions.append(\"\")\n",
    "\n",
    "    return scraped_links, scraped_titles, scraped_descriptions\n",
    "\n",
    "# Example usage\n",
    "dawn_links, dawn_titles, dawn_descriptions = scrape_data('https://www.dawn.com/')\n",
    "bbc_links, bbc_titles, bbc_descriptions = scrape_data('https://www.bbc.com/')\n",
    "\n",
    "# Print the extracted data\n",
    "print(\"Scraped Dawn Links:\")\n",
    "print(dawn_links)\n",
    "print(\"\\nScraped Dawn Titles:\")\n",
    "print(dawn_titles)\n",
    "print(\"\\nScraped Dawn Descriptions:\")\n",
    "print(dawn_descriptions)\n",
    "\n",
    "print(\"\\nScraped BBC Links:\")\n",
    "print(bbc_links)\n",
    "print(\"\\nScraped BBC Titles:\")\n",
    "print(bbc_titles)\n",
    "print(\"\\nScraped BBC Descriptions:\")\n",
    "print(bbc_descriptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_lowercase(text_data):\n",
    "    \"\"\"\n",
    "    Cleans the text data by removing special characters and digits, and converts it to lowercase.\n",
    "\n",
    "    Args:\n",
    "        text_data (list): A list of strings containing the data to clean and lowercase.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cleaned and lowercased strings.\n",
    "    \"\"\"\n",
    "    cleaned_data = []\n",
    "    for item in text_data:\n",
    "        # Remove special characters and digits\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', item)\n",
    "        # Convert to lowercase\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "        cleaned_data.append(cleaned_text)\n",
    "    return cleaned_data\n",
    "\n",
    "# Preprocess the extracted data\n",
    "dawn_titles_cleaned = clean_and_lowercase(dawn_titles)\n",
    "dawn_descriptions_cleaned = clean_and_lowercase(dawn_descriptions)\n",
    "bbc_titles_cleaned = clean_and_lowercase(bbc_titles)\n",
    "bbc_descriptions_cleaned = clean_and_lowercase(bbc_descriptions)\n",
    "\n",
    "# Print the preprocessed data\n",
    "print(\"\\nCleaned Dawn Titles:\")\n",
    "print(dawn_titles_cleaned)\n",
    "print(\"\\nCleaned Dawn Descriptions:\")\n",
    "print(dawn_descriptions_cleaned)\n",
    "\n",
    "print(\"\\nCleaned BBC Titles:\")\n",
    "print(bbc_titles_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Preprocess titles and descriptions\n",
    "def preprocess_and_collect_data_titles_and_descriptions(titles, descriptions):\n",
    "    preprocessed_titles = []\n",
    "    preprocessed_descriptions = []\n",
    "    for title, description in zip(titles, descriptions):\n",
    "        preprocessed_title_stemmed, preprocessed_title_lemmatized = preprocess_text(title)\n",
    "        preprocessed_description_stemmed, preprocessed_description_lemmatized = preprocess_text(description)\n",
    "        preprocessed_titles.append((preprocessed_title_stemmed, preprocessed_title_lemmatized))\n",
    "        preprocessed_descriptions.append((preprocessed_description_stemmed, preprocessed_description_lemmatized))\n",
    "    return preprocessed_titles, preprocessed_descriptions\n",
    "\n",
    "preprocessed_dawn_titles, preprocessed_dawn_descriptions = preprocess_and_collect_data_titles_and_descriptions(dawn_titles, dawn_descriptions)\n",
    "preprocessed_bbc_titles, preprocessed_bbc_descriptions = preprocess_and_collect_data_titles_and_descriptions(bbc_titles, bbc_descriptions)\n",
    "\n",
    "# Write the preprocessed data into CSV\n",
    "def write_preprocessed_data_to_csv(preprocessed_titles, preprocessed_descriptions, website_name):\n",
    "    with open('dataExtracted.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for title, description in zip(preprocessed_titles, preprocessed_descriptions):\n",
    "            writer.writerow([website_name, title, description])\n",
    "\n",
    "write_preprocessed_data_to_csv(preprocessed_dawn_titles, preprocessed_dawn_descriptions, 'Dawn')\n",
    "write_preprocessed_data_to_csv(preprocessed_bbc_titles, preprocessed_bbc_descriptions, 'BBC')\n",
    "\n",
    "print(\"Data has been saved to 'dataExtracted.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Function to scrape links, titles, and descriptions from a webpage\n",
    "def scrape_webpage_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extracting links, titles, and descriptions\n",
    "    links = []\n",
    "    titles = []\n",
    "    descriptions = []\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href and href.startswith('http'):\n",
    "            links.append(href)\n",
    "            title = link.get_text(strip=True)\n",
    "            titles.append(title)\n",
    "            # Extracting description if available\n",
    "            description = link.find_next_sibling(string=True)\n",
    "            if description and len(description.strip()) > 0:\n",
    "                descriptions.append(description.strip())\n",
    "            else:\n",
    "                descriptions.append(\"\")\n",
    "    \n",
    "    return links, titles, descriptions\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<[^<]+?>', '', text)\n",
    "    # Remove punctuation and special characters\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Stemming and Lemmatization\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return stemmed_tokens, lemmatized_tokens\n",
    "\n",
    "# Example usage\n",
    "dawn_links, dawn_titles, dawn_descriptions = scrape_webpage_data('https://www.dawn.com/')\n",
    "bbc_links, bbc_titles, bbc_descriptions = scrape_webpage_data('https://www.bbc.com/')\n",
    "\n",
    "# Preprocess titles and descriptions\n",
    "preprocessed_dawn_titles = []\n",
    "preprocessed_dawn_descriptions = []\n",
    "for title, description in zip(dawn_titles, dawn_descriptions):\n",
    "    preprocessed_title_stemmed, preprocessed_title_lemmatized = preprocess_text(title)\n",
    "    preprocessed_description_stemmed, preprocessed_description_lemmatized = preprocess_text(description)\n",
    "    preprocessed_dawn_titles.append((preprocessed_title_stemmed, preprocessed_title_lemmatized))\n",
    "    preprocessed_dawn_descriptions.append((preprocessed_description_stemmed, preprocessed_description_lemmatized))\n",
    "\n",
    "preprocessed_bbc_titles = []\n",
    "preprocessed_bbc_descriptions = []\n",
    "for title, description in zip(bbc_titles, bbc_descriptions):\n",
    "    preprocessed_title_stemmed, preprocessed_title_lemmatized = preprocess_text(title)\n",
    "    preprocessed_description_stemmed, preprocessed_description_lemmatized = preprocess_text(description)\n",
    "    preprocessed_bbc_titles.append((preprocessed_title_stemmed, preprocessed_title_lemmatized))\n",
    "    preprocessed_bbc_descriptions.append((preprocessed_description_stemmed, preprocessed_description_lemmatized))\n",
    "\n",
    "# Write the preprocessed data into CSV\n",
    "with open('dataExtracted.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['Website', 'Title', 'Lemmatized Title', 'Description', 'Lemmatized Description']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write Dawn data\n",
    "    for title, description in zip(preprocessed_dawn_titles, preprocessed_dawn_descriptions):\n",
    "        writer.writerow({'Website': 'Dawn', \n",
    "                         'Title': ' '.join(title[0]),  # Join stemmed tokens\n",
    "                         'Lemmatized Title': ' '.join(title[1]),  # Join lemmatized tokens\n",
    "                         'Description': ' '.join(description[0]),  # Join stemmed tokens\n",
    "                         'Lemmatized Description': ' '.join(description[1])})  # Join lemmatized tokens\n",
    "\n",
    "    # Write BBC data\n",
    "    for title, description in zip(preprocessed_bbc_titles, preprocessed_bbc_descriptions):\n",
    "        writer.writerow({'Website': 'BBC', \n",
    "                         'Title': ' '.join(title[0]),  # Join stemmed tokens\n",
    "                         'Lemmatized Title': ' '.join(title[1]),  # Join lemmatized tokens\n",
    "                         'Description': ' '.join(description[0]),  # Join stemmed tokens\n",
    "                         'Lemmatized Description': ' '.join(description[1])})  # Join lemmatized tokens\n",
    "\n",
    "print(\"Data has been saved to 'dataExtracted.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
